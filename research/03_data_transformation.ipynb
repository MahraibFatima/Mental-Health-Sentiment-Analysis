{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# config.yaml\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MentalHealthAnalysis import logger\n",
    "from src.MentalHealthAnalysis.constants import *\n",
    "from src.MentalHealthAnalysis.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        try:\n",
    "            config = self.config.get('data_transformation', {})\n",
    "            create_directories([config.get('root_dir', '')])\n",
    "\n",
    "            data_transformation_config = DataTransformationConfig(\n",
    "                root_dir=config.get('root_dir', ''),\n",
    "                data_path=config.get('data_path', ''),\n",
    "            )\n",
    "            return data_transformation_config\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error in data transformation config: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import hstack\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def read_data(self):\n",
    "        \"\"\"Reads the dataset from the specified path.\"\"\"\n",
    "        self.data = pd.read_csv(self.config.data_path)\n",
    "    \n",
    "    def preprocessing(self):\n",
    "        \"\"\"Performs preprocessing steps such as handling missing values and text cleaning.\"\"\"\n",
    "        self.data.dropna(inplace=True)\n",
    "\n",
    "        # Reset index after dropping rows\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Create additional features based on text length and number of sentences\n",
    "        self.data['num_of_characters'] = self.data['statement'].str.len()\n",
    "        self.data['num_of_sentences'] = self.data['statement'].apply(lambda x: len(sent_tokenize(x)))\n",
    "        \n",
    "        # Lowercasing the text\n",
    "        self.data['statement'] = self.data['statement'].str.lower()\n",
    "    \n",
    "    def remove_patterns(self, text):\n",
    "        \"\"\"Removes unwanted patterns such as URLs, markdown links, handles, and punctuation.\"\"\"\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def clean_text(self):\n",
    "        \"\"\"Applies the `remove_patterns` function to clean the text.\"\"\"\n",
    "        self.data['statement'] = self.data['statement'].apply(self.remove_patterns)\n",
    "    \n",
    "    def tokenize_and_stem(self):\n",
    "        \"\"\"Tokenizes and stems the statements.\"\"\"\n",
    "        self.data['tokens'] = self.data['statement'].apply(word_tokenize)\n",
    "        \n",
    "        # Initialize the stemmer\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        # Function to stem tokens\n",
    "        def stem_tokens(tokens):\n",
    "            return [stemmer.stem(token) for token in tokens]  # Return list of tokens\n",
    "        \n",
    "        self.data['tokens_stemmed'] = self.data['tokens'].apply(stem_tokens)\n",
    "\n",
    "    def vectorize_text(self):\n",
    "        \"\"\"Vectorizes the text using TF-IDF and combines it with numerical features.\"\"\"\n",
    "    \n",
    "        # Print columns to debug\n",
    "\n",
    "        # Label encoding target variable 'y'\n",
    "        #target_columns = ['Anxiety', 'Bipolar', 'Depression', 'Personality disorder', 'Stress', 'Suicidal']\n",
    "    \n",
    "        # Check if target columns exist\n",
    "        # if not all(col in self.data.columns for col in target_columns):\n",
    "        #     raise KeyError(f\"One or more target columns {target_columns} are not found in the dataset.\")\n",
    "    \n",
    "        #y = self.data[target_columns]\n",
    "\n",
    "        # Convert multi-label to single label (argmax across rows)\n",
    "        #y_single = y.values.argmax(axis=1)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        y_onehot = onehot_encoder.fit_transform(self.data['status'].values)\n",
    "        self.data = y_onehot.argmax(axis=1)\n",
    "\n",
    "        print(f\"Columns in the dataset: {self.data.columns}\")\n",
    "\n",
    "        # Drop the target columns from X\n",
    "        X = self.data.drop(target_columns, axis=1)\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_single, test_size=0.2, random_state=101)\n",
    "\n",
    "        # Initialize TF-IDF Vectorizer\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=50000)\n",
    "\n",
    "        # Fit and transform TF-IDF on training data\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train['tokens_stemmed'])\n",
    "        X_test_tfidf = vectorizer.transform(X_test['tokens_stemmed'])\n",
    "\n",
    "        # Extract numerical features\n",
    "        X_train_num = X_train[['num_of_characters', 'num_of_sentences']].values\n",
    "        X_test_num = X_test[['num_of_characters', 'num_of_sentences']].values\n",
    "\n",
    "        # Combine TF-IDF and numerical features\n",
    "        X_train_combined = hstack([X_train_tfidf, X_train_num])\n",
    "        X_test_combined = hstack([X_test_tfidf, X_test_num])\n",
    "\n",
    "        print(f'Number of feature words: {len(vectorizer.get_feature_names_out())}')\n",
    "\n",
    "        # Apply Random Over-Sampling on the vectorized data\n",
    "        ros = RandomOverSampler(random_state=101)\n",
    "        X_train_resampled, y_train_resampled = ros.fit_resample(X_train_combined, y_train)\n",
    "\n",
    "        print(f\"Resampled training data shape: {X_train_resampled.shape}\")\n",
    "        return X_train_resampled, X_test_combined, y_train_resampled, y_test\n",
    "\n",
    "\n",
    "    def train_test_split_save(self, X_train_resampled, X_test_combined, y_train_resampled, y_test):\n",
    "        \"\"\"Saves the processed train/test sets to the specified directory.\"\"\"\n",
    "        print(\"Saving transformed training and test sets...\")\n",
    "\n",
    "        # Combine X_train_resampled and y_train_resampled\n",
    "        train = pd.DataFrame(X_train_resampled.toarray(), columns=[f'feature_{i}' for i in range(X_train_resampled.shape[1])])\n",
    "        train['target'] = y_train_resampled.values\n",
    "\n",
    "        # Combine X_test_combined and y_test\n",
    "        test = pd.DataFrame(X_test_combined.toarray(), columns=[f'feature_{i}' for i in range(X_test_combined.shape[1])])\n",
    "        test['target'] = y_test.values\n",
    "\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(self.config.processed_data_dir, exist_ok=True)\n",
    "    \n",
    "        # Save to CSV\n",
    "        train.to_csv(os.path.join(self.config.processed_data_dir, \"train.csv\"), index=False)\n",
    "        test.to_csv(os.path.join(self.config.processed_data_dir, \"test.csv\"), index=False)\n",
    "\n",
    "        print(f\"Training data saved to {os.path.join(self.config.processed_data_dir, 'train.csv')}\")\n",
    "        print(f\"Testing data saved to {os.path.join(self.config.processed_data_dir, 'test.csv')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    \n",
    "    # Call methods using the instance of the class, not the class name\n",
    "    data_transformation.read_data()\n",
    "    data_transformation.preprocessing()\n",
    "    \n",
    "    # Apply remove_patterns inside the clean_text method (since it’s already applied there)\n",
    "    data_transformation.clean_text()\n",
    "    \n",
    "    # Tokenize and stem the text\n",
    "    data_transformation.tokenize_and_stem()\n",
    "    \n",
    "    # Perform vectorization and resampling\n",
    "    X_train_resampled, X_test_combined, y_train_resampled, y_test = data_transformation.vectorize_text()\n",
    "    \n",
    "    # Split and save the train/test data\n",
    "    data_transformation.train_test_split_save(X_train_resampled, X_test_combined, y_train_resampled, y_test)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
